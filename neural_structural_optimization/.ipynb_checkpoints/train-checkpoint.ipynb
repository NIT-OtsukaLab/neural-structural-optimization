{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lint as python3\n",
    "Copyright 2019 Google LLC.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pylint: disable=missing-docstring\n",
    "# pylint: disable=superfluous-parens\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import logging\n",
    "import autograd\n",
    "import autograd.numpy as np\n",
    "from neural_structural_optimization import models\n",
    "from neural_structural_optimization import topo_physics\n",
    "import scipy.optimize\n",
    "import tensorflow as tf\n",
    "import xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer_result_dataset(losses, frames, save_intermediate_designs=False):\n",
    "  # The best design will often but not always be the final one.\n",
    "  best_design = np.nanargmin(losses)\n",
    "  logging.info(f'Final loss: {losses[best_design]}')\n",
    "  if save_intermediate_designs:\n",
    "    ds = xarray.Dataset({\n",
    "        'loss': (('step',), losses),\n",
    "        'design': (('step', 'z', 'y', 'x'), frames),\n",
    "    }, coords={'step': np.arange(len(losses))})\n",
    "  else:\n",
    "    ds = xarray.Dataset({\n",
    "        'loss': (('step',), losses),\n",
    "        'design': (('z', 'y', 'x'), frames[best_design]),\n",
    "    }, coords={'step': np.arange(len(losses))})\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tf_optimizer(\n",
    "    model, max_iterations, optimizer, save_intermediate_designs=True,\n",
    "):\n",
    "  loss = 0\n",
    "  model(None)  # build model, if not built\n",
    "  tvars = model.trainable_variables\n",
    "\n",
    "  losses = []\n",
    "  frames = []\n",
    "  for i in range(max_iterations + 1):\n",
    "    with tf.GradientTape() as t:\n",
    "      t.watch(tvars)\n",
    "      logits = model(None)\n",
    "      loss = model.loss(logits)\n",
    "\n",
    "    losses.append(loss.numpy().item())\n",
    "    frames.append(logits.numpy())\n",
    "\n",
    "    if i % (max_iterations // 10) == 0:\n",
    "      logging.info(f'step {i}, loss {losses[-1]:.2f}')\n",
    "\n",
    "    if i < max_iterations:\n",
    "      grads = t.gradient(loss, tvars)\n",
    "      optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "  designs = [model.env.render(x, volume_contraint=True) for x in frames]\n",
    "  return optimizer_result_dataset(np.array(losses), np.array(designs),\n",
    "                                  save_intermediate_designs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_adam = functools.partial(\n",
    "    train_tf_optimizer, optimizer=tf.keras.optimizers.Adam(1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_variables(variables, x):\n",
    "  print(\"i\")\n",
    "  shapes = [v.shape.as_list() for v in variables]\n",
    "  values = tf.split(x, [np.prod(s) for s in shapes])\n",
    "  for var, value in zip(variables, values):\n",
    "    var.assign(tf.reshape(tf.cast(value, var.dtype), var.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_variables(variables):\n",
    "  return np.concatenate([\n",
    "      v.numpy().ravel() if not isinstance(v, np.ndarray) else v.ravel()\n",
    "      for v in variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lbfgs(\n",
    "    model, max_iterations, save_intermediate_designs=True, init_model=None,\n",
    "    **kwargs\n",
    "):\n",
    "  model(None)  # build model, if not built\n",
    "\n",
    "  losses = []\n",
    "  frames = []\n",
    "\n",
    "  if init_model is not None:\n",
    "    if not isinstance(model, models.PixelModel):\n",
    "      raise TypeError('can only use init_model for initializing a PixelModel')\n",
    "    model.z.assign(tf.cast(init_model(None), model.z.dtype))\n",
    "\n",
    "  tvars = model.trainable_variables\n",
    "\n",
    "  def value_and_grad(x):\n",
    "    _set_variables(tvars, x)\n",
    "    with tf.GradientTape() as t:\n",
    "      t.watch(tvars)\n",
    "      logits = model(None)\n",
    "      loss = model.loss(logits)\n",
    "    grads = t.gradient(loss, tvars)\n",
    "    frames.append(logits.numpy().copy())\n",
    "    losses.append(loss.numpy().copy())\n",
    "    return float(loss.numpy()), _get_variables(grads).astype(np.float64)\n",
    "\n",
    "  x0 = _get_variables(tvars).astype(np.float64)\n",
    "  # rely upon the step limit instead of error tolerance for finishing.\n",
    "  _, _, info = scipy.optimize.fmin_l_bfgs_b(\n",
    "      value_and_grad, x0, maxfun=max_iterations, factr=1, pgtol=1e-14, **kwargs\n",
    "  )\n",
    "  logging.info(info)\n",
    "\n",
    "  designs = [model.env.render(x, volume_contraint=True) for x in frames]\n",
    "  return optimizer_result_dataset(\n",
    "      np.array(losses), np.array(designs), save_intermediate_designs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constrained_logits(init_model):\n",
    "  \"\"\"Produce matching initial conditions with volume constraints applied.\"\"\"\n",
    "  logits = init_model(None).numpy().astype(np.float64).squeeze(axis=0)\n",
    "  return topo_physics.physical_density(logits, init_model.env.args, volume_contraint=True, cone_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method_of_moving_asymptotes(model, max_iterations, save_intermediate_designs=True, init_model=None,):\n",
    "  import nlopt  # pylint: disable=g-import-not-at-top\n",
    "\n",
    "  if not isinstance(model, models.PixelModel):\n",
    "    raise ValueError('MMA only defined for pixel models')\n",
    "\n",
    "  env = model.env\n",
    "  if init_model is None:\n",
    "    x0 = _get_variables(model.trainable_variables).astype(np.float64)\n",
    "  else:\n",
    "    x0 = constrained_logits(init_model).ravel()\n",
    "\n",
    "  def objective(x):\n",
    "    return env.objective(x, volume_contraint=False)\n",
    "\n",
    "  def constraint(x):\n",
    "    return env.constraint(x)\n",
    "\n",
    "  def wrap_autograd_func(func, losses=None, frames=None):\n",
    "    def wrapper(x, grad):\n",
    "      if grad.size > 0:\n",
    "        value, grad[:] = autograd.value_and_grad(func)(x)\n",
    "      else:\n",
    "        value = func(x)\n",
    "      if losses is not None:\n",
    "        losses.append(value)\n",
    "      if frames is not None:\n",
    "        frames.append(env.reshape(x).copy())\n",
    "      return value\n",
    "    return wrapper\n",
    "\n",
    "  losses = []\n",
    "  frames = []\n",
    "\n",
    "  opt = nlopt.opt(nlopt.LD_MMA, x0.size)\n",
    "  opt.set_lower_bounds(0.0)\n",
    "  opt.set_upper_bounds(1.0)\n",
    "  opt.set_min_objective(wrap_autograd_func(objective, losses, frames))\n",
    "  opt.add_inequality_constraint(wrap_autograd_func(constraint), 1e-8)\n",
    "  opt.set_maxeval(max_iterations + 1)\n",
    "  opt.optimize(x0)\n",
    "\n",
    "  designs = [env.render(x, volume_contraint=False) for x in frames]\n",
    "  return optimizer_result_dataset(np.array(losses), np.array(designs), save_intermediate_designs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimality_criteria(model, max_iterations, save_intermediate_designs=True, init_model=None,):\n",
    "  if not isinstance(model, models.PixelModel):\n",
    "    raise ValueError('optimality criteria only defined for pixel models')\n",
    "\n",
    "  env = model.env\n",
    "  if init_model is None:\n",
    "    x = _get_variables(model.trainable_variables).astype(np.float64)\n",
    "  else:\n",
    "    x = constrained_logits(init_model).ravel()\n",
    "\n",
    "  # start with the first frame but not its loss, since optimality_criteria_step\n",
    "  # returns the current loss and the *next* design.\n",
    "  losses = []\n",
    "  frames = [x]\n",
    "  for _ in range(max_iterations):\n",
    "    c, x = topo_physics.optimality_criteria_step(x, env.ke, env.args)\n",
    "    losses.append(c)\n",
    "    if np.isnan(c):\n",
    "      # no point in continuing to optimize\n",
    "      break\n",
    "    frames.append(x)\n",
    "  losses.append(env.objective(x, volume_contraint=False))\n",
    "\n",
    "  designs = [env.render(x, volume_contraint=False) for x in frames]\n",
    "  return optimizer_result_dataset(np.array(losses), np.array(designs),\n",
    "                                  save_intermediate_designs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(model_list, flag_values, train_func=train_adam):\n",
    "  batch_hist = []\n",
    "  for batch_ix in range(flag_values.trials):\n",
    "    logging.info(f'Starting trial {batch_ix}')\n",
    "    history = train_func(model_list[batch_ix], flag_values)\n",
    "    batch_hist.append(history)\n",
    "\n",
    "  batch_hist = xarray.concat(batch_hist, dim='batch')\n",
    "  return batch_hist"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
